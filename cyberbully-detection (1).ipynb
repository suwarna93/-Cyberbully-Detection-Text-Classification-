{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install texthero==1.0.5","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:40:15.820402Z","iopub.execute_input":"2022-05-15T06:40:15.820727Z","iopub.status.idle":"2022-05-15T06:40:27.681753Z","shell.execute_reply.started":"2022-05-15T06:40:15.820636Z","shell.execute_reply":"2022-05-15T06:40:27.680849Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install spacy 1.3.1\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:40:27.684188Z","iopub.execute_input":"2022-05-15T06:40:27.684482Z","iopub.status.idle":"2022-05-15T06:40:29.534923Z","shell.execute_reply.started":"2022-05-15T06:40:27.684441Z","shell.execute_reply":"2022-05-15T06:40:29.534109Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pip install transformers==2.11.0","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:40:29.536389Z","iopub.execute_input":"2022-05-15T06:40:29.536691Z","iopub.status.idle":"2022-05-15T06:40:43.583385Z","shell.execute_reply.started":"2022-05-15T06:40:29.536660Z","shell.execute_reply":"2022-05-15T06:40:43.582416Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install spacy-transformers[cuda100]==0.6.2","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:40:43.585414Z","iopub.execute_input":"2022-05-15T06:40:43.585616Z","iopub.status.idle":"2022-05-15T06:41:09.267005Z","shell.execute_reply.started":"2022-05-15T06:40:43.585591Z","shell.execute_reply":"2022-05-15T06:41:09.266212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pip install -U textblob","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:09.268335Z","iopub.execute_input":"2022-05-15T06:41:09.268582Z","iopub.status.idle":"2022-05-15T06:41:18.488197Z","shell.execute_reply.started":"2022-05-15T06:41:09.268552Z","shell.execute_reply":"2022-05-15T06:41:18.487348Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:18.489863Z","iopub.execute_input":"2022-05-15T06:41:18.490127Z","iopub.status.idle":"2022-05-15T06:41:28.304611Z","shell.execute_reply.started":"2022-05-15T06:41:18.490098Z","shell.execute_reply":"2022-05-15T06:41:28.303667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pip install spacy 1.3.1","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:28.307190Z","iopub.execute_input":"2022-05-15T06:41:28.307549Z","iopub.status.idle":"2022-05-15T06:41:30.638144Z","shell.execute_reply.started":"2022-05-15T06:41:28.307503Z","shell.execute_reply":"2022-05-15T06:41:30.636930Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\nimport re\nimport string\nfrom wordcloud import WordCloud\n\nfrom textblob import TextBlob\n\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.models import word2vec\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:30.640062Z","iopub.execute_input":"2022-05-15T06:41:30.640344Z","iopub.status.idle":"2022-05-15T06:41:32.643211Z","shell.execute_reply.started":"2022-05-15T06:41:30.640305Z","shell.execute_reply":"2022-05-15T06:41:32.642403Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\ndf.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.644761Z","iopub.execute_input":"2022-05-15T06:41:32.645032Z","iopub.status.idle":"2022-05-15T06:41:32.756849Z","shell.execute_reply.started":"2022-05-15T06:41:32.644996Z","shell.execute_reply":"2022-05-15T06:41:32.756005Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **#Exploratory Data Analysis** (Sentence Level Analysis)","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.760585Z","iopub.execute_input":"2022-05-15T06:41:32.761172Z","iopub.status.idle":"2022-05-15T06:41:32.791614Z","shell.execute_reply.started":"2022-05-15T06:41:32.761138Z","shell.execute_reply":"2022-05-15T06:41:32.790331Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.793540Z","iopub.execute_input":"2022-05-15T06:41:32.793896Z","iopub.status.idle":"2022-05-15T06:41:32.803091Z","shell.execute_reply.started":"2022-05-15T06:41:32.793841Z","shell.execute_reply":"2022-05-15T06:41:32.801971Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.804775Z","iopub.execute_input":"2022-05-15T06:41:32.805816Z","iopub.status.idle":"2022-05-15T06:41:32.823218Z","shell.execute_reply.started":"2022-05-15T06:41:32.805773Z","shell.execute_reply":"2022-05-15T06:41:32.822281Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.825023Z","iopub.execute_input":"2022-05-15T06:41:32.825364Z","iopub.status.idle":"2022-05-15T06:41:32.855081Z","shell.execute_reply.started":"2022-05-15T06:41:32.825322Z","shell.execute_reply":"2022-05-15T06:41:32.854321Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df['oh_label']=df['oh_label'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.856039Z","iopub.execute_input":"2022-05-15T06:41:32.856255Z","iopub.status.idle":"2022-05-15T06:41:32.861326Z","shell.execute_reply.started":"2022-05-15T06:41:32.856231Z","shell.execute_reply":"2022-05-15T06:41:32.860510Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df.drop(['index','id','Annotation'], axis = 1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.863157Z","iopub.execute_input":"2022-05-15T06:41:32.863703Z","iopub.status.idle":"2022-05-15T06:41:32.873251Z","shell.execute_reply.started":"2022-05-15T06:41:32.863658Z","shell.execute_reply":"2022-05-15T06:41:32.872395Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.874375Z","iopub.execute_input":"2022-05-15T06:41:32.875687Z","iopub.status.idle":"2022-05-15T06:41:32.889519Z","shell.execute_reply.started":"2022-05-15T06:41:32.875645Z","shell.execute_reply":"2022-05-15T06:41:32.888563Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# ### First, I’ll take a look at the number of characters present in each sentence. This can give us a rough idea about the text length.","metadata":{}},{"cell_type":"code","source":"df['Text'].str.len().hist()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:32.891297Z","iopub.execute_input":"2022-05-15T06:41:32.891906Z","iopub.status.idle":"2022-05-15T06:41:33.293740Z","shell.execute_reply.started":"2022-05-15T06:41:32.891859Z","shell.execute_reply":"2022-05-15T06:41:33.292895Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### To check the sentence length distribution.","metadata":{}},{"cell_type":"code","source":"df['len']= df['Text'].str.len()\nprint('Max length: {}, Min length: {}, Average Length :  {}'.format(max(df['len']),min(df['len']),df['len'].mean()))\ndf['len'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:33.295424Z","iopub.execute_input":"2022-05-15T06:41:33.295728Z","iopub.status.idle":"2022-05-15T06:41:33.545750Z","shell.execute_reply.started":"2022-05-15T06:41:33.295687Z","shell.execute_reply":"2022-05-15T06:41:33.544842Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### Word level Analysis","metadata":{}},{"cell_type":"code","source":"text = ','.join([str(i) for i in df['Text']])\nwords_list= text.split()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:33.547937Z","iopub.execute_input":"2022-05-15T06:41:33.548515Z","iopub.status.idle":"2022-05-15T06:41:33.585760Z","shell.execute_reply.started":"2022-05-15T06:41:33.548470Z","shell.execute_reply":"2022-05-15T06:41:33.584904Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"word_freq= {}\nfor word in set(words_list):\n    word_freq[word]= words_list.count(word)\n#Creating dataframe of words\ndf_word= pd.DataFrame(word_freq.items(),columns=['word','count'])","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:41:33.588178Z","iopub.execute_input":"2022-05-15T06:41:33.588428Z","iopub.status.idle":"2022-05-15T06:44:20.900178Z","shell.execute_reply.started":"2022-05-15T06:41:33.588394Z","shell.execute_reply":"2022-05-15T06:44:20.899328Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_word['word_len']= df_word['word'].map(lambda x: len(x))\n# sorting values \ndf_word=df_word.sort_values('count',ascending=False).reset_index(drop=True)\ndf_word","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:20.901432Z","iopub.execute_input":"2022-05-15T06:44:20.901706Z","iopub.status.idle":"2022-05-15T06:44:20.953274Z","shell.execute_reply.started":"2022-05-15T06:44:20.901670Z","shell.execute_reply":"2022-05-15T06:44:20.952326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_top= df_word.head(50)\nsns.barplot(df_top['count'],df_top['word'])","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:20.954521Z","iopub.execute_input":"2022-05-15T06:44:20.954870Z","iopub.status.idle":"2022-05-15T06:44:21.744188Z","shell.execute_reply.started":"2022-05-15T06:44:20.954832Z","shell.execute_reply":"2022-05-15T06:44:21.743407Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_word['word_len'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:21.745401Z","iopub.execute_input":"2022-05-15T06:44:21.746320Z","iopub.status.idle":"2022-05-15T06:44:21.956740Z","shell.execute_reply.started":"2022-05-15T06:44:21.746277Z","shell.execute_reply":"2022-05-15T06:44:21.955877Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'].sample(1).values[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:21.958067Z","iopub.execute_input":"2022-05-15T06:44:21.958394Z","iopub.status.idle":"2022-05-15T06:44:21.966351Z","shell.execute_reply.started":"2022-05-15T06:44:21.958340Z","shell.execute_reply":"2022-05-15T06:44:21.965627Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df['oh_label'].value_counts().plot(kind='bar', color=sns.color_palette('pastel'))\nplt.xticks([0,1],['toxic', 'non-toxic'], rotation=0);","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:21.967508Z","iopub.execute_input":"2022-05-15T06:44:21.968219Z","iopub.status.idle":"2022-05-15T06:44:22.145981Z","shell.execute_reply.started":"2022-05-15T06:44:21.968177Z","shell.execute_reply":"2022-05-15T06:44:22.145208Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### ###Analyzing the amount and the types of stopwords can give us some good insights into the data.","metadata":{}},{"cell_type":"markdown","source":"#### We will use the counter function from the collections library to count and store the occurrences of each word in a list of tuples. This is a very useful function when we deal with word-level analysis in natural language processing.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"code","source":"for index,text in enumerate(df['Text'][ :100]):\n  print('Review %d:\\n'%(index+1),text)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:22.147198Z","iopub.execute_input":"2022-05-15T06:44:22.147749Z","iopub.status.idle":"2022-05-15T06:44:22.184008Z","shell.execute_reply.started":"2022-05-15T06:44:22.147707Z","shell.execute_reply":"2022-05-15T06:44:22.183326Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### Here, you can see that we have some contractions like “It’s”, numbers like “3” and punctuations like “,”, “!” and “.” present in the reviews. We’ll handle these by performing the below operations:\n\n1) Expand contractions\n2) Lowercase the reviews\n3) Remove digits and words containing digits\n4) Remove punctuations","metadata":{}},{"cell_type":"code","source":"df['cleaned']=df['Text'].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:22.185391Z","iopub.execute_input":"2022-05-15T06:44:22.185628Z","iopub.status.idle":"2022-05-15T06:44:22.199532Z","shell.execute_reply.started":"2022-05-15T06:44:22.185595Z","shell.execute_reply":"2022-05-15T06:44:22.198607Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df['cleaned']=df['cleaned'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:22.206342Z","iopub.execute_input":"2022-05-15T06:44:22.206853Z","iopub.status.idle":"2022-05-15T06:44:22.734236Z","shell.execute_reply.started":"2022-05-15T06:44:22.206820Z","shell.execute_reply":"2022-05-15T06:44:22.733400Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\ndf['cleaned']=df['cleaned'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:22.738933Z","iopub.execute_input":"2022-05-15T06:44:22.741010Z","iopub.status.idle":"2022-05-15T06:44:22.926567Z","shell.execute_reply.started":"2022-05-15T06:44:22.740956Z","shell.execute_reply":"2022-05-15T06:44:22.925759Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\n# Removing extra spaces\ndf['cleaned']=df['cleaned'].apply(lambda x: re.sub(' +',' ',x))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:22.928120Z","iopub.execute_input":"2022-05-15T06:44:22.928378Z","iopub.status.idle":"2022-05-15T06:44:23.052018Z","shell.execute_reply.started":"2022-05-15T06:44:22.928342Z","shell.execute_reply":"2022-05-15T06:44:23.051270Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"                                                      \ndf['cleaned']=df['cleaned'].apply(lambda x:re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                                \"]+\",flags=re.UNICODE).sub(r'', x)\n                                  \n)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:23.053275Z","iopub.execute_input":"2022-05-15T06:44:23.054032Z","iopub.status.idle":"2022-05-15T06:44:23.202589Z","shell.execute_reply.started":"2022-05-15T06:44:23.053993Z","shell.execute_reply":"2022-05-15T06:44:23.201847Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df['corrected_text'] = 'cleaned'\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:23.203899Z","iopub.execute_input":"2022-05-15T06:44:23.204246Z","iopub.status.idle":"2022-05-15T06:44:23.220847Z","shell.execute_reply.started":"2022-05-15T06:44:23.204209Z","shell.execute_reply":"2022-05-15T06:44:23.219974Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for index,corrected_text in enumerate(df['cleaned'][:100]):\n  print('Review %d:\\n'%(index+1),corrected_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:23.222132Z","iopub.execute_input":"2022-05-15T06:44:23.222462Z","iopub.status.idle":"2022-05-15T06:44:23.255761Z","shell.execute_reply.started":"2022-05-15T06:44:23.222424Z","shell.execute_reply":"2022-05-15T06:44:23.255117Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# ####We’ll use SpaCy for the removal of stopwords and lemmatization. It is a library for advanced Natural Language Processing in Python","metadata":{}},{"cell_type":"code","source":"# Importing spacy\nimport spacy\n\n# Loading model\nnlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n\n# Lemmatization with stopwords removal\ndf['lemmatized']=df['cleaned'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:45:13.443215Z","iopub.execute_input":"2022-05-15T06:45:13.443533Z","iopub.status.idle":"2022-05-15T06:46:21.566958Z","shell.execute_reply.started":"2022-05-15T06:45:13.443500Z","shell.execute_reply":"2022-05-15T06:46:21.566171Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df_grouped=df[['Text','lemmatized']].groupby(by='Text').agg(lambda x:' '.join(x))\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:36.129696Z","iopub.execute_input":"2022-05-15T06:46:36.129992Z","iopub.status.idle":"2022-05-15T06:46:36.304680Z","shell.execute_reply.started":"2022-05-15T06:46:36.129939Z","shell.execute_reply":"2022-05-15T06:46:36.303837Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Creating Document Term Matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(analyzer='word')\ndata=cv.fit_transform(df_grouped['lemmatized'])\ndf_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\ndf_dtm.index=df_grouped.index\ndf_dtm.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:39.023248Z","iopub.execute_input":"2022-05-15T06:46:39.024226Z","iopub.status.idle":"2022-05-15T06:46:39.575398Z","shell.execute_reply.started":"2022-05-15T06:46:39.024172Z","shell.execute_reply":"2022-05-15T06:46:39.574664Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"#### So, let’s start by looking at the common words present in the reviews for each product. For this, I will use the document term matrix created earlier with word clouds for plotting these words. Word clouds are the visual representations of the frequency of different words present in a document.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:42.630821Z","iopub.execute_input":"2022-05-15T06:46:42.631566Z","iopub.status.idle":"2022-05-15T06:46:42.709932Z","shell.execute_reply.started":"2022-05-15T06:46:42.631526Z","shell.execute_reply":"2022-05-15T06:46:42.709044Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nnew= df['Text'].str.split()\nnew=new.values.tolist()\ncorpus=[word for i in new for word in i]\n\nfrom collections import defaultdict\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:44.203879Z","iopub.execute_input":"2022-05-15T06:46:44.204501Z","iopub.status.idle":"2022-05-15T06:46:44.316257Z","shell.execute_reply.started":"2022-05-15T06:46:44.204460Z","shell.execute_reply":"2022-05-15T06:46:44.315363Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\n            \n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    x,y=zip(*top)\n    plt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:46.439828Z","iopub.execute_input":"2022-05-15T06:46:46.440592Z","iopub.status.idle":"2022-05-15T06:46:46.676138Z","shell.execute_reply.started":"2022-05-15T06:46:46.440553Z","shell.execute_reply":"2022-05-15T06:46:46.675394Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"### Plot top non stopwords\nfrom collections import  Counter\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx, y= [], []\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:49.017844Z","iopub.execute_input":"2022-05-15T06:46:49.018569Z","iopub.status.idle":"2022-05-15T06:46:49.327147Z","shell.execute_reply.started":"2022-05-15T06:46:49.018527Z","shell.execute_reply":"2022-05-15T06:46:49.326328Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# ##Wordcloud\nWordcloud is a great way to represent text data. The size and color of each word that appears in the wordcloud indicate it’s frequency or importance.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30,\n        scale=3,\n        random_state=1)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:51.966204Z","iopub.execute_input":"2022-05-15T06:46:51.966809Z","iopub.status.idle":"2022-05-15T06:46:54.783996Z","shell.execute_reply.started":"2022-05-15T06:46:51.966767Z","shell.execute_reply":"2022-05-15T06:46:54.783284Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### Textblob\nTextblob is a python library built on top of nltk. It has been around for some time and is very easy and convenient to use.\n\nThe sentiment function of TextBlob returns two properties:\n\npolarity: is a floating-point number that lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement.\nsubjectivity: refers to how someone’s judgment is shaped by personal opinions and feelings. Subjectivity is represented as a floating-point value which lies in the range of [0,1].","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:46:59.496795Z","iopub.execute_input":"2022-05-15T06:46:59.497551Z","iopub.status.idle":"2022-05-15T06:46:59.502379Z","shell.execute_reply.started":"2022-05-15T06:46:59.497512Z","shell.execute_reply":"2022-05-15T06:46:59.501096Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"TextBlob(df['Text'][8]).sentiment","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:47:01.110693Z","iopub.execute_input":"2022-05-15T06:47:01.111340Z","iopub.status.idle":"2022-05-15T06:47:01.169732Z","shell.execute_reply.started":"2022-05-15T06:47:01.111298Z","shell.execute_reply":"2022-05-15T06:47:01.168982Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndf['polarity_score']=df['Text'].\\\n   apply(lambda x : polarity(x))\ndf['polarity_score'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:47:02.797800Z","iopub.execute_input":"2022-05-15T06:47:02.798691Z","iopub.status.idle":"2022-05-15T06:47:07.193715Z","shell.execute_reply.started":"2022-05-15T06:47:02.798645Z","shell.execute_reply":"2022-05-15T06:47:07.192839Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def sentiment(x):\n    if x==0:\n        return 'toxic'\n    else:\n        return 'non-toxic'\n    \ndf['polarity']=df['polarity_score'].\\\n   map(lambda x: sentiment(x))\n\nplt.bar(df.polarity.value_counts().index,\n        df.polarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:48:58.207230Z","iopub.execute_input":"2022-05-15T06:48:58.207531Z","iopub.status.idle":"2022-05-15T06:48:58.383967Z","shell.execute_reply.started":"2022-05-15T06:48:58.207501Z","shell.execute_reply":"2022-05-15T06:48:58.382926Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"x = ['i feel happy']\nsentiment(x)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:49:05.008340Z","iopub.execute_input":"2022-05-15T06:49:05.009108Z","iopub.status.idle":"2022-05-15T06:49:05.014594Z","shell.execute_reply.started":"2022-05-15T06:49:05.009065Z","shell.execute_reply":"2022-05-15T06:49:05.013840Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"#### Let’s take a look at some of the toxic and non-toxic tweets","metadata":{}},{"cell_type":"code","source":"df[df['polarity']=='toxic']['cleaned'].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:28.919426Z","iopub.execute_input":"2022-05-15T06:50:28.920039Z","iopub.status.idle":"2022-05-15T06:50:28.936256Z","shell.execute_reply.started":"2022-05-15T06:50:28.919998Z","shell.execute_reply":"2022-05-15T06:50:28.935018Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"df[df['polarity']=='non-toxic']['cleaned'].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:32.217793Z","iopub.execute_input":"2022-05-15T06:50:32.218340Z","iopub.status.idle":"2022-05-15T06:50:32.232588Z","shell.execute_reply.started":"2022-05-15T06:50:32.218302Z","shell.execute_reply":"2022-05-15T06:50:32.231822Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:34.747498Z","iopub.execute_input":"2022-05-15T06:50:34.747788Z","iopub.status.idle":"2022-05-15T06:50:34.770855Z","shell.execute_reply.started":"2022-05-15T06:50:34.747757Z","shell.execute_reply":"2022-05-15T06:50:34.770023Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Extraction\nSome features will be extracted after text cleaning because they are more meaningful to obtain at this step","metadata":{}},{"cell_type":"code","source":"df['word_count'] = df['cleaned'].apply(lambda x: len(str(x).split(\" \")))\ndf[['cleaned','word_count']].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:39.866458Z","iopub.execute_input":"2022-05-15T06:50:39.866737Z","iopub.status.idle":"2022-05-15T06:50:39.911322Z","shell.execute_reply.started":"2022-05-15T06:50:39.866706Z","shell.execute_reply":"2022-05-15T06:50:39.910501Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df['char_count'] = df['cleaned'].str.len() ## this also includes spaces\ndf[['cleaned','char_count']].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:42.627556Z","iopub.execute_input":"2022-05-15T06:50:42.627842Z","iopub.status.idle":"2022-05-15T06:50:42.651773Z","shell.execute_reply.started":"2022-05-15T06:50:42.627809Z","shell.execute_reply":"2022-05-15T06:50:42.650927Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)/(len(words)+0.000001))\ndf['avg_word'] = df['cleaned'].apply(lambda x: avg_word(x)).round(1)\ndf[['cleaned','avg_word']].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:45.716552Z","iopub.execute_input":"2022-05-15T06:50:45.717421Z","iopub.status.idle":"2022-05-15T06:50:45.794399Z","shell.execute_reply.started":"2022-05-15T06:50:45.717376Z","shell.execute_reply":"2022-05-15T06:50:45.793364Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:48.150254Z","iopub.execute_input":"2022-05-15T06:50:48.150526Z","iopub.status.idle":"2022-05-15T06:50:48.168342Z","shell.execute_reply.started":"2022-05-15T06:50:48.150498Z","shell.execute_reply":"2022-05-15T06:50:48.167087Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#### Modelling","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:44:34.112738Z","iopub.status.idle":"2022-05-15T06:44:34.113363Z","shell.execute_reply.started":"2022-05-15T06:44:34.113128Z","shell.execute_reply":"2022-05-15T06:44:34.113152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = df.drop(columns=['Text', 'corrected_text', 'len', 'lemmatized','polarity_score','polarity','word_count','char_count','avg_word'])","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:50.844043Z","iopub.execute_input":"2022-05-15T06:50:50.844608Z","iopub.status.idle":"2022-05-15T06:50:50.850650Z","shell.execute_reply.started":"2022-05-15T06:50:50.844569Z","shell.execute_reply":"2022-05-15T06:50:50.849394Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"final_df","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:53.131390Z","iopub.execute_input":"2022-05-15T06:50:53.133772Z","iopub.status.idle":"2022-05-15T06:50:53.146750Z","shell.execute_reply.started":"2022-05-15T06:50:53.133721Z","shell.execute_reply":"2022-05-15T06:50:53.145856Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"X = final_df['cleaned']\ny = final_df['oh_label']","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:56.673493Z","iopub.execute_input":"2022-05-15T06:50:56.674329Z","iopub.status.idle":"2022-05-15T06:50:56.679784Z","shell.execute_reply.started":"2022-05-15T06:50:56.674292Z","shell.execute_reply":"2022-05-15T06:50:56.678881Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:50:58.146702Z","iopub.execute_input":"2022-05-15T06:50:58.147278Z","iopub.status.idle":"2022-05-15T06:50:58.154286Z","shell.execute_reply.started":"2022-05-15T06:50:58.147240Z","shell.execute_reply":"2022-05-15T06:50:58.153454Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:00.897395Z","iopub.execute_input":"2022-05-15T06:51:00.897993Z","iopub.status.idle":"2022-05-15T06:51:00.904109Z","shell.execute_reply.started":"2022-05-15T06:51:00.897936Z","shell.execute_reply":"2022-05-15T06:51:00.903346Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:03.138292Z","iopub.execute_input":"2022-05-15T06:51:03.139235Z","iopub.status.idle":"2022-05-15T06:51:03.143873Z","shell.execute_reply.started":"2022-05-15T06:51:03.139188Z","shell.execute_reply":"2022-05-15T06:51:03.143195Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"cv = CountVectorizer()\nX = cv.fit_transform(X) # Fit the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:05.701672Z","iopub.execute_input":"2022-05-15T06:51:05.701932Z","iopub.status.idle":"2022-05-15T06:51:06.064305Z","shell.execute_reply.started":"2022-05-15T06:51:05.701902Z","shell.execute_reply":"2022-05-15T06:51:06.062667Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#Naive Bayes Classifier\nclf = MultinomialNB()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:07.859957Z","iopub.execute_input":"2022-05-15T06:51:07.860432Z","iopub.status.idle":"2022-05-15T06:51:07.890866Z","shell.execute_reply.started":"2022-05-15T06:51:07.860397Z","shell.execute_reply":"2022-05-15T06:51:07.889861Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:10.332047Z","iopub.execute_input":"2022-05-15T06:51:10.332400Z","iopub.status.idle":"2022-05-15T06:51:10.366278Z","shell.execute_reply.started":"2022-05-15T06:51:10.332363Z","shell.execute_reply":"2022-05-15T06:51:10.365428Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# ### Save the Model For Deployment","metadata":{}},{"cell_type":"code","source":"model = MultinomialNB()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:17.145771Z","iopub.execute_input":"2022-05-15T06:51:17.146289Z","iopub.status.idle":"2022-05-15T06:51:17.150406Z","shell.execute_reply.started":"2022-05-15T06:51:17.146248Z","shell.execute_reply":"2022-05-15T06:51:17.149628Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:18.394994Z","iopub.execute_input":"2022-05-15T06:51:18.395679Z","iopub.status.idle":"2022-05-15T06:51:18.409267Z","shell.execute_reply.started":"2022-05-15T06:51:18.395625Z","shell.execute_reply":"2022-05-15T06:51:18.408190Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# fitting model\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:20.937691Z","iopub.execute_input":"2022-05-15T06:51:20.938012Z","iopub.status.idle":"2022-05-15T06:51:20.953172Z","shell.execute_reply.started":"2022-05-15T06:51:20.937970Z","shell.execute_reply":"2022-05-15T06:51:20.952018Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# predicting\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:23.306012Z","iopub.execute_input":"2022-05-15T06:51:23.306570Z","iopub.status.idle":"2022-05-15T06:51:23.311732Z","shell.execute_reply.started":"2022-05-15T06:51:23.306532Z","shell.execute_reply":"2022-05-15T06:51:23.310842Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:26.051046Z","iopub.execute_input":"2022-05-15T06:51:26.051632Z","iopub.status.idle":"2022-05-15T06:51:26.077270Z","shell.execute_reply.started":"2022-05-15T06:51:26.051591Z","shell.execute_reply":"2022-05-15T06:51:26.076365Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"score = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", score)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:30.874864Z","iopub.execute_input":"2022-05-15T06:51:30.875442Z","iopub.status.idle":"2022-05-15T06:51:30.882504Z","shell.execute_reply.started":"2022-05-15T06:51:30.875404Z","shell.execute_reply":"2022-05-15T06:51:30.881642Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"import pickle \npickle_out = open(\"model.pkl\", mode = \"wb\") \npickle.dump(model, pickle_out) \npickle_out.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:51:32.002201Z","iopub.execute_input":"2022-05-15T06:51:32.002477Z","iopub.status.idle":"2022-05-15T06:51:32.008666Z","shell.execute_reply.started":"2022-05-15T06:51:32.002448Z","shell.execute_reply":"2022-05-15T06:51:32.007794Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}